==============================================
CSDI Trajectory Generator
==============================================
Job ID: 5179516
Node: gpu004
Started: Thu Jan  1 13:19:10 EST 2026

Python: /opt/sw/spack/apps/linux-rhel8-x86_64_v2/gcc-10.3.0/miniconda3-22.11.1-gy/bin/python
PyTorch: 2.9.1+cu128
CUDA available: True

Starting training...
Config: 6 layers, d_ff=1024, cosine schedule, smoothness_weight=0.1
============================================================
CSDI Trajectory Generator - Training
============================================================
Device: cuda
Epochs: 200
Diffusion steps: 200
Loading 6367 trajectory files...
Loaded 5991 valid trajectories
Model parameters: 5,496,065
Epoch 1: Loss = 0.3465, LR = 1.00e-04
Epoch 2: Loss = 0.3233, LR = 1.00e-04
Epoch 3: Loss = 0.3118, LR = 9.99e-05
Epoch 4: Loss = 0.3012, LR = 9.99e-05
Epoch 5: Loss = 0.2978, LR = 9.98e-05
Epoch 6: Loss = 0.3067, LR = 9.98e-05
Epoch 7: Loss = 0.3048, LR = 9.97e-05
Epoch 8: Loss = 0.2987, LR = 9.96e-05
Epoch 9: Loss = 0.2949, LR = 9.95e-05
Epoch 10: Loss = 0.2773, LR = 9.94e-05
Epoch 11: Loss = 0.2611, LR = 9.93e-05
Epoch 12: Loss = 0.2541, LR = 9.91e-05
Epoch 13: Loss = 0.2498, LR = 9.90e-05
Epoch 14: Loss = 0.2473, LR = 9.88e-05
Epoch 15: Loss = 0.2393, LR = 9.86e-05
Epoch 16: Loss = 0.2372, LR = 9.84e-05
Epoch 17: Loss = 0.2361, LR = 9.82e-05
Epoch 18: Loss = 0.2346, LR = 9.80e-05
Epoch 19: Loss = 0.2314, LR = 9.78e-05
Epoch 20: Loss = 0.2310, LR = 9.76e-05
Epoch 21: Loss = 0.2314, LR = 9.73e-05
Epoch 22: Loss = 0.2344, LR = 9.70e-05
Epoch 23: Loss = 0.2298, LR = 9.68e-05
Epoch 24: Loss = 0.2286, LR = 9.65e-05
Epoch 25: Loss = 0.2266, LR = 9.62e-05
Epoch 26: Loss = 0.2234, LR = 9.59e-05
Epoch 27: Loss = 0.2231, LR = 9.56e-05
Epoch 28: Loss = 0.2241, LR = 9.52e-05
Epoch 29: Loss = 0.2209, LR = 9.49e-05
Epoch 30: Loss = 0.2216, LR = 9.46e-05
Epoch 31: Loss = 0.2229, LR = 9.42e-05
Epoch 32: Loss = 0.2209, LR = 9.38e-05
Epoch 33: Loss = 0.2253, LR = 9.34e-05
Epoch 34: Loss = 0.2241, LR = 9.30e-05
Epoch 35: Loss = 0.2207, LR = 9.26e-05
Epoch 36: Loss = 0.2198, LR = 9.22e-05
Epoch 37: Loss = 0.2189, LR = 9.18e-05
Epoch 38: Loss = 0.2207, LR = 9.14e-05
Epoch 39: Loss = 0.2177, LR = 9.09e-05
Epoch 40: Loss = 0.2210, LR = 9.05e-05
Epoch 41: Loss = 0.2191, LR = 9.00e-05
Epoch 42: Loss = 0.2187, LR = 8.95e-05
Epoch 43: Loss = 0.2187, LR = 8.90e-05
Epoch 44: Loss = 0.2167, LR = 8.85e-05
Epoch 45: Loss = 0.2204, LR = 8.80e-05
Epoch 46: Loss = 0.2172, LR = 8.75e-05
Epoch 47: Loss = 0.2177, LR = 8.70e-05
Epoch 48: Loss = 0.2157, LR = 8.64e-05
Epoch 49: Loss = 0.2167, LR = 8.59e-05
Epoch 50: Loss = 0.2167, LR = 8.54e-05
Epoch 51: Loss = 0.2158, LR = 8.48e-05
Epoch 52: Loss = 0.2186, LR = 8.42e-05
Epoch 53: Loss = 0.2168, LR = 8.37e-05
Epoch 54: Loss = 0.2149, LR = 8.31e-05
Epoch 55: Loss = 0.2176, LR = 8.25e-05
Epoch 56: Loss = 0.2149, LR = 8.19e-05
Epoch 57: Loss = 0.2160, LR = 8.13e-05
Epoch 58: Loss = 0.2151, LR = 8.06e-05
Epoch 59: Loss = 0.2129, LR = 8.00e-05
Epoch 60: Loss = 0.2188, LR = 7.94e-05
Epoch 61: Loss = 0.2147, LR = 7.88e-05
Epoch 62: Loss = 0.2149, LR = 7.81e-05
Epoch 63: Loss = 0.2136, LR = 7.75e-05
Epoch 64: Loss = 0.2158, LR = 7.68e-05
Epoch 65: Loss = 0.2125, LR = 7.61e-05
Epoch 66: Loss = 0.2128, LR = 7.55e-05
Epoch 67: Loss = 0.2140, LR = 7.48e-05
Epoch 68: Loss = 0.2131, LR = 7.41e-05
Epoch 69: Loss = 0.2125, LR = 7.34e-05
Epoch 70: Loss = 0.2113, LR = 7.27e-05
Epoch 71: Loss = 0.2167, LR = 7.20e-05
Epoch 72: Loss = 0.2151, LR = 7.13e-05
Epoch 73: Loss = 0.2118, LR = 7.06e-05
Epoch 74: Loss = 0.2139, LR = 6.99e-05
Epoch 75: Loss = 0.2141, LR = 6.91e-05
Epoch 76: Loss = 0.2135, LR = 6.84e-05
Epoch 77: Loss = 0.2149, LR = 6.77e-05
Epoch 78: Loss = 0.2137, LR = 6.69e-05
Epoch 79: Loss = 0.2123, LR = 6.62e-05
Epoch 80: Loss = 0.2101, LR = 6.55e-05
Epoch 81: Loss = 0.2098, LR = 6.47e-05
Epoch 82: Loss = 0.2114, LR = 6.39e-05
Epoch 83: Loss = 0.2130, LR = 6.32e-05
Epoch 84: Loss = 0.2098, LR = 6.24e-05
Epoch 85: Loss = 0.2115, LR = 6.17e-05
Epoch 86: Loss = 0.2109, LR = 6.09e-05
Epoch 87: Loss = 0.2092, LR = 6.01e-05
Epoch 88: Loss = 0.2085, LR = 5.94e-05
Epoch 89: Loss = 0.2085, LR = 5.86e-05
Epoch 90: Loss = 0.2106, LR = 5.78e-05
Epoch 91: Loss = 0.2118, LR = 5.70e-05
Epoch 92: Loss = 0.2089, LR = 5.63e-05
Epoch 93: Loss = 0.2116, LR = 5.55e-05
Epoch 94: Loss = 0.2121, LR = 5.47e-05
Epoch 95: Loss = 0.2112, LR = 5.39e-05
Epoch 96: Loss = 0.2101, LR = 5.31e-05
Epoch 97: Loss = 0.2087, LR = 5.24e-05
Epoch 98: Loss = 0.2099, LR = 5.16e-05
Epoch 99: Loss = 0.2092, LR = 5.08e-05
Epoch 100: Loss = 0.2106, LR = 5.00e-05
Epoch 101: Loss = 0.2097, LR = 4.92e-05
Epoch 102: Loss = 0.2093, LR = 4.84e-05
Epoch 103: Loss = 0.2085, LR = 4.76e-05
Epoch 104: Loss = 0.2072, LR = 4.69e-05
Epoch 105: Loss = 0.2087, LR = 4.61e-05
Epoch 106: Loss = 0.2107, LR = 4.53e-05
Epoch 107: Loss = 0.2071, LR = 4.45e-05
Epoch 108: Loss = 0.2086, LR = 4.37e-05
Epoch 109: Loss = 0.2090, LR = 4.30e-05
Epoch 110: Loss = 0.2087, LR = 4.22e-05
Epoch 111: Loss = 0.2093, LR = 4.14e-05
Epoch 112: Loss = 0.2080, LR = 4.06e-05
Epoch 113: Loss = 0.2114, LR = 3.99e-05
Epoch 114: Loss = 0.2089, LR = 3.91e-05
Epoch 115: Loss = 0.2084, LR = 3.83e-05
Epoch 116: Loss = 0.2072, LR = 3.76e-05
Epoch 117: Loss = 0.2103, LR = 3.68e-05
Epoch 118: Loss = 0.2056, LR = 3.61e-05
Epoch 119: Loss = 0.2093, LR = 3.53e-05
Epoch 120: Loss = 0.2077, LR = 3.45e-05
Epoch 121: Loss = 0.2068, LR = 3.38e-05
Epoch 122: Loss = 0.2070, LR = 3.31e-05
Epoch 123: Loss = 0.2070, LR = 3.23e-05
Epoch 124: Loss = 0.2077, LR = 3.16e-05
Epoch 125: Loss = 0.2079, LR = 3.09e-05
Epoch 126: Loss = 0.2071, LR = 3.01e-05
Epoch 127: Loss = 0.2082, LR = 2.94e-05
Epoch 128: Loss = 0.2069, LR = 2.87e-05
Epoch 129: Loss = 0.2077, LR = 2.80e-05
Epoch 130: Loss = 0.2075, LR = 2.73e-05
Epoch 131: Loss = 0.2036, LR = 2.66e-05
Epoch 132: Loss = 0.2065, LR = 2.59e-05
Epoch 133: Loss = 0.2066, LR = 2.52e-05
Epoch 134: Loss = 0.2077, LR = 2.45e-05
Epoch 135: Loss = 0.2047, LR = 2.39e-05
Epoch 136: Loss = 0.2059, LR = 2.32e-05
Epoch 137: Loss = 0.2051, LR = 2.25e-05
Epoch 138: Loss = 0.2058, LR = 2.19e-05
Epoch 139: Loss = 0.2045, LR = 2.12e-05
Epoch 140: Loss = 0.2060, LR = 2.06e-05
Epoch 141: Loss = 0.2060, LR = 2.00e-05
Epoch 142: Loss = 0.2067, LR = 1.94e-05
Epoch 143: Loss = 0.2042, LR = 1.87e-05
Epoch 144: Loss = 0.2062, LR = 1.81e-05
Epoch 145: Loss = 0.2058, LR = 1.75e-05
Epoch 146: Loss = 0.2063, LR = 1.69e-05
Epoch 147: Loss = 0.2043, LR = 1.63e-05
Epoch 148: Loss = 0.2059, LR = 1.58e-05
Epoch 149: Loss = 0.2051, LR = 1.52e-05
Epoch 150: Loss = 0.2064, LR = 1.46e-05
Epoch 151: Loss = 0.2057, LR = 1.41e-05
Epoch 152: Loss = 0.2038, LR = 1.36e-05
Epoch 153: Loss = 0.2060, LR = 1.30e-05
Epoch 154: Loss = 0.2049, LR = 1.25e-05
Epoch 155: Loss = 0.2042, LR = 1.20e-05
Epoch 156: Loss = 0.2043, LR = 1.15e-05
Epoch 157: Loss = 0.2037, LR = 1.10e-05
Epoch 158: Loss = 0.2042, LR = 1.05e-05
Epoch 159: Loss = 0.2047, LR = 1.00e-05
Epoch 160: Loss = 0.2043, LR = 9.55e-06
Epoch 161: Loss = 0.2051, LR = 9.09e-06
Epoch 162: Loss = 0.2053, LR = 8.65e-06
Epoch 163: Loss = 0.2059, LR = 8.21e-06
Epoch 164: Loss = 0.2046, LR = 7.78e-06
Epoch 165: Loss = 0.2037, LR = 7.37e-06
Epoch 166: Loss = 0.2042, LR = 6.96e-06
Epoch 167: Loss = 0.2049, LR = 6.57e-06
Epoch 168: Loss = 0.2037, LR = 6.18e-06
Epoch 169: Loss = 0.2031, LR = 5.81e-06
Epoch 170: Loss = 0.2041, LR = 5.45e-06
Epoch 171: Loss = 0.2043, LR = 5.10e-06
Epoch 172: Loss = 0.2036, LR = 4.76e-06
Epoch 173: Loss = 0.2043, LR = 4.43e-06
Epoch 174: Loss = 0.2023, LR = 4.11e-06
Epoch 175: Loss = 0.2052, LR = 3.81e-06
Epoch 176: Loss = 0.2050, LR = 3.51e-06
Epoch 177: Loss = 0.2028, LR = 3.23e-06
Epoch 178: Loss = 0.2040, LR = 2.96e-06
Epoch 179: Loss = 0.2046, LR = 2.70e-06
Epoch 180: Loss = 0.2060, LR = 2.45e-06
Epoch 181: Loss = 0.2046, LR = 2.21e-06
Epoch 182: Loss = 0.2043, LR = 1.99e-06
Epoch 183: Loss = 0.2040, LR = 1.77e-06
Epoch 184: Loss = 0.2060, LR = 1.57e-06
Epoch 185: Loss = 0.2029, LR = 1.38e-06
Epoch 186: Loss = 0.2045, LR = 1.20e-06
Epoch 187: Loss = 0.2039, LR = 1.04e-06
Epoch 188: Loss = 0.2037, LR = 8.86e-07
Epoch 189: Loss = 0.2039, LR = 7.45e-07
Epoch 190: Loss = 0.2043, LR = 6.16e-07
Epoch 191: Loss = 0.2044, LR = 4.99e-07
Epoch 192: Loss = 0.2042, LR = 3.94e-07
Epoch 193: Loss = 0.2022, LR = 3.02e-07
Epoch 194: Loss = 0.2035, LR = 2.22e-07
Epoch 195: Loss = 0.2041, LR = 1.54e-07
Epoch 196: Loss = 0.2042, LR = 9.87e-08
Epoch 197: Loss = 0.2045, LR = 5.55e-08
Epoch 198: Loss = 0.2029, LR = 2.47e-08
Epoch 199: Loss = 0.2047, LR = 6.17e-09
Epoch 200: Loss = 0.2051, LR = 0.00e+00
Training complete. Best loss: 0.2022

Starting generation...
============================================================
CSDI Trajectory Generator - Generation
============================================================
Loading real data for condition sampling...
Warning: Could not load real data, using uniform sampling
Generating 1000 trajectories...
Diffusion steps: 200, Smoothing kernel: 7
Saved samples plot: csdi_samples_20260101_141606.png

==============================================
Completed: Thu Jan  1 14:16:08 EST 2026
==============================================
